{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import model_selection\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(500)\n",
    "\n",
    "df = pd.read_csv('model/moral-data.csv', sep=';')\n",
    "df.drop(['deontic_modality', 'type'], axis=1)\n",
    "df.rename(columns={'general_rule': 'labels'}, inplace=True)\n",
    "\n",
    "# Step 1: Remove blank rows if any.\n",
    "df['text'].dropna(inplace=True)\n",
    "\n",
    "# Step 2: Change all the text to lower case\n",
    "df['text'] = [entry.lower() for entry in df['text']]\n",
    "\n",
    "# Step 3: Remove Stop words, Non-Numeric and perfom Word Stemming/Lemmenting\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "\n",
    "for index, entry in enumerate(df['text']):\n",
    "    final_words = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for word, tag in pos_tag(entry):\n",
    "        if word not in stopwords.words('english') and word.isalpha():\n",
    "            final_word = lemmatizer.lemmatize(word, tag_map[tag[0]])\n",
    "            final_words.append(final_word)\n",
    "    df.loc[index,'text'] = str(final_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               512512    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 1026      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 513,538\n",
      "Trainable params: 513,538\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 63 samples, validate on 7 samples\n",
      "Epoch 1/80\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.6946 - acc: 0.4762 - val_loss: 0.7072 - val_acc: 0.4286\n",
      "Epoch 2/80\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.6274 - acc: 0.6190 - val_loss: 0.6998 - val_acc: 0.4286\n",
      "Epoch 3/80\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.5842 - acc: 0.6984 - val_loss: 0.6894 - val_acc: 0.2857\n",
      "Epoch 4/80\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.5438 - acc: 0.7302 - val_loss: 0.6946 - val_acc: 0.2857\n",
      "Epoch 5/80\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.5049 - acc: 0.7619 - val_loss: 0.7079 - val_acc: 0.4286\n",
      "Epoch 6/80\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.4822 - acc: 0.7778 - val_loss: 0.7043 - val_acc: 0.2857\n",
      "Epoch 7/80\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.4769 - acc: 0.7778 - val_loss: 0.7231 - val_acc: 0.2857\n",
      "Epoch 8/80\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.4561 - acc: 0.7619 - val_loss: 0.7611 - val_acc: 0.4286\n",
      "Epoch 9/80\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.4422 - acc: 0.8413 - val_loss: 0.7313 - val_acc: 0.2857\n",
      "Epoch 10/80\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.4289 - acc: 0.8095 - val_loss: 0.7954 - val_acc: 0.4286\n",
      "Epoch 11/80\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.4358 - acc: 0.7778 - val_loss: 0.7804 - val_acc: 0.1429\n",
      "Epoch 12/80\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.4325 - acc: 0.8095 - val_loss: 0.7551 - val_acc: 0.4286\n",
      "Epoch 13/80\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.4082 - acc: 0.8095 - val_loss: 0.7940 - val_acc: 0.2857\n",
      "Epoch 14/80\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.3861 - acc: 0.8254 - val_loss: 0.8003 - val_acc: 0.2857\n",
      "Epoch 15/80\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.3908 - acc: 0.7778 - val_loss: 0.7921 - val_acc: 0.4286\n",
      "Epoch 16/80\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.3965 - acc: 0.8254 - val_loss: 0.8494 - val_acc: 0.4286\n",
      "Epoch 17/80\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.3625 - acc: 0.8571 - val_loss: 0.7895 - val_acc: 0.4286\n",
      "Epoch 18/80\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.3503 - acc: 0.8730 - val_loss: 0.8068 - val_acc: 0.4286\n",
      "Epoch 19/80\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2960 - acc: 0.8889 - val_loss: 0.8294 - val_acc: 0.4286\n",
      "Epoch 20/80\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.3173 - acc: 0.8571 - val_loss: 0.8478 - val_acc: 0.4286\n",
      "Epoch 21/80\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.2898 - acc: 0.8730 - val_loss: 0.9035 - val_acc: 0.4286\n",
      "Epoch 22/80\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 0.3157 - acc: 0.8413 - val_loss: 0.9276 - val_acc: 0.4286\n",
      "Epoch 23/80\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.2772 - acc: 0.9048 - val_loss: 0.8936 - val_acc: 0.4286\n",
      "Epoch 24/80\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.3177 - acc: 0.8730 - val_loss: 0.9683 - val_acc: 0.4286\n",
      "Epoch 25/80\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.3083 - acc: 0.8889 - val_loss: 0.9714 - val_acc: 0.4286\n",
      "Epoch 26/80\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2824 - acc: 0.9048 - val_loss: 1.0248 - val_acc: 0.4286\n",
      "Epoch 27/80\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2702 - acc: 0.8571 - val_loss: 0.9372 - val_acc: 0.5714\n",
      "Epoch 28/80\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2884 - acc: 0.8889 - val_loss: 1.0035 - val_acc: 0.5714\n",
      "Epoch 29/80\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2450 - acc: 0.9206 - val_loss: 0.9975 - val_acc: 0.5714\n",
      "Epoch 30/80\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2571 - acc: 0.8571 - val_loss: 1.0668 - val_acc: 0.5714\n",
      "Epoch 31/80\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2438 - acc: 0.9048 - val_loss: 1.0438 - val_acc: 0.5714\n",
      "Epoch 32/80\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2384 - acc: 0.9048 - val_loss: 1.0919 - val_acc: 0.5714\n",
      "Epoch 33/80\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2432 - acc: 0.9048 - val_loss: 1.0889 - val_acc: 0.5714\n",
      "Epoch 34/80\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2318 - acc: 0.8889 - val_loss: 1.0709 - val_acc: 0.7143\n",
      "Epoch 35/80\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2078 - acc: 0.9365 - val_loss: 1.1660 - val_acc: 0.5714\n",
      "Epoch 36/80\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2289 - acc: 0.9048 - val_loss: 1.1511 - val_acc: 0.5714\n",
      "Epoch 37/80\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2405 - acc: 0.9048 - val_loss: 1.1551 - val_acc: 0.7143\n",
      "Epoch 38/80\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2248 - acc: 0.9048 - val_loss: 1.2190 - val_acc: 0.7143\n",
      "Epoch 39/80\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2050 - acc: 0.8889 - val_loss: 1.1883 - val_acc: 0.7143\n",
      "Epoch 40/80\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2084 - acc: 0.9048 - val_loss: 1.1709 - val_acc: 0.7143\n",
      "Epoch 41/80\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.2323 - acc: 0.9048 - val_loss: 1.2306 - val_acc: 0.7143\n",
      "Epoch 42/80\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.2122 - acc: 0.8889 - val_loss: 1.3114 - val_acc: 0.4286\n",
      "Epoch 43/80\n",
      "63/63 [==============================] - 1s 8ms/step - loss: 0.2372 - acc: 0.8889 - val_loss: 1.2766 - val_acc: 0.5714\n",
      "Epoch 44/80\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.2271 - acc: 0.8571 - val_loss: 1.2686 - val_acc: 0.7143\n",
      "Epoch 45/80\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.2248 - acc: 0.9206 - val_loss: 1.3551 - val_acc: 0.4286\n",
      "Epoch 46/80\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.2186 - acc: 0.9048 - val_loss: 1.2814 - val_acc: 0.7143\n",
      "Epoch 47/80\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2024 - acc: 0.9048 - val_loss: 1.3623 - val_acc: 0.4286\n",
      "Epoch 48/80\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2065 - acc: 0.8730 - val_loss: 1.3386 - val_acc: 0.7143\n",
      "Epoch 49/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1863 - acc: 0.9206 - val_loss: 1.3384 - val_acc: 0.5714\n",
      "Epoch 50/80\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2073 - acc: 0.8889 - val_loss: 1.3489 - val_acc: 0.5714\n",
      "Epoch 51/80\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.2196 - acc: 0.9206 - val_loss: 1.4525 - val_acc: 0.5714\n",
      "Epoch 52/80\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1716 - acc: 0.9206 - val_loss: 1.4118 - val_acc: 0.7143\n",
      "Epoch 53/80\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.2002 - acc: 0.8730 - val_loss: 1.3212 - val_acc: 0.7143\n",
      "Epoch 54/80\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1846 - acc: 0.9048 - val_loss: 1.4996 - val_acc: 0.4286\n",
      "Epoch 55/80\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1834 - acc: 0.8889 - val_loss: 1.4160 - val_acc: 0.5714\n",
      "Epoch 56/80\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1656 - acc: 0.9206 - val_loss: 1.4659 - val_acc: 0.5714\n",
      "Epoch 57/80\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1817 - acc: 0.9206 - val_loss: 1.4091 - val_acc: 0.7143\n",
      "Epoch 58/80\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.2075 - acc: 0.9048 - val_loss: 1.4180 - val_acc: 0.7143\n",
      "Epoch 59/80\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1999 - acc: 0.8730 - val_loss: 1.4239 - val_acc: 0.7143\n",
      "Epoch 60/80\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.2225 - acc: 0.8889 - val_loss: 1.4208 - val_acc: 0.7143\n",
      "Epoch 61/80\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1580 - acc: 0.9365 - val_loss: 1.4944 - val_acc: 0.7143\n",
      "Epoch 62/80\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1366 - acc: 0.9524 - val_loss: 1.4690 - val_acc: 0.7143\n",
      "Epoch 63/80\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1998 - acc: 0.8889 - val_loss: 1.4585 - val_acc: 0.7143\n",
      "Epoch 64/80\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1576 - acc: 0.8889 - val_loss: 1.4672 - val_acc: 0.7143\n",
      "Epoch 65/80\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1865 - acc: 0.9206 - val_loss: 1.5570 - val_acc: 0.5714\n",
      "Epoch 66/80\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1743 - acc: 0.9048 - val_loss: 1.5372 - val_acc: 0.7143\n",
      "Epoch 67/80\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1699 - acc: 0.9206 - val_loss: 1.6137 - val_acc: 0.5714\n",
      "Epoch 68/80\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 0.1731 - acc: 0.9206 - val_loss: 1.5914 - val_acc: 0.5714\n",
      "Epoch 69/80\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1635 - acc: 0.9048 - val_loss: 1.5466 - val_acc: 0.7143\n",
      "Epoch 70/80\n",
      "63/63 [==============================] - 1s 8ms/step - loss: 0.1791 - acc: 0.9048 - val_loss: 1.5985 - val_acc: 0.5714\n",
      "Epoch 71/80\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1664 - acc: 0.9365 - val_loss: 1.5601 - val_acc: 0.7143\n",
      "Epoch 72/80\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1840 - acc: 0.9365 - val_loss: 1.6049 - val_acc: 0.5714\n",
      "Epoch 73/80\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1668 - acc: 0.9048 - val_loss: 1.5906 - val_acc: 0.7143\n",
      "Epoch 74/80\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1602 - acc: 0.9048 - val_loss: 1.5524 - val_acc: 0.7143\n",
      "Epoch 75/80\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1691 - acc: 0.9048 - val_loss: 1.5799 - val_acc: 0.7143\n",
      "Epoch 76/80\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1479 - acc: 0.9206 - val_loss: 1.5446 - val_acc: 0.7143\n",
      "Epoch 77/80\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1697 - acc: 0.9048 - val_loss: 1.5938 - val_acc: 0.7143\n",
      "Epoch 78/80\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1647 - acc: 0.9048 - val_loss: 1.5299 - val_acc: 0.7143\n",
      "Epoch 79/80\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1458 - acc: 0.9048 - val_loss: 1.5881 - val_acc: 0.5714\n",
      "Epoch 80/80\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1504 - acc: 0.9206 - val_loss: 1.5307 - val_acc: 0.7143\n"
     ]
    }
   ],
   "source": [
    "train_X, test_X, train_Y, test_Y = model_selection.train_test_split(df['text'],df['labels'],test_size=0.2)\n",
    "\n",
    "max_words = 1000\n",
    "tokenize = text.Tokenizer(num_words=max_words, char_level=False)\n",
    "tokenize.fit_on_texts(train_X) # only fit on train\n",
    "\n",
    "train_X = tokenize.texts_to_matrix(train_X)\n",
    "test_X = tokenize.texts_to_matrix(test_X)\n",
    "\n",
    "num_classes = np.max(train_Y) + 1\n",
    "train_Y = utils.to_categorical(train_Y, num_classes)\n",
    "test_Y = utils.to_categorical(test_Y, num_classes)\n",
    "\n",
    "batch_size = 2\n",
    "epochs = 80\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_words,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "              \n",
    "history = model.fit(train_X, train_Y,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    verbose=1,\n",
    "    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(test_X, test_Y, batch_size=batch_size, verbose=1)\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f3dcd5937e01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'return the ring'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpredicted_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(tokenize.texts_to_matrix('return the ring'))\n",
    "predicted_label = np.argmax(prediction[0])\n",
    "print(predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimieren: https://realpython.com/python-keras-text-classification/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
